{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdb723b-f3f6-4d2f-afb2-463ed3adf325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ba6422-18c1-42db-b745-a6d687e65a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Enhanced Data Generator (Fix bugs & normalize labels)\n",
    "\n",
    "class AgeGenderRaceDataGenerator(Sequence):\n",
    "    def __init__(self, file_list, folder_path, batch_size=32, image_size=(96, 96), shuffle=True):\n",
    "        self.file_list = file_list\n",
    "        self.folder_path = folder_path\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.file_list) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_files = self.file_list[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        images, ages, genders, races = [], [], [], []\n",
    "\n",
    "        for fname in batch_files:\n",
    "            img_path = os.path.join(self.folder_path, fname)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            img = cv2.resize(img, self.image_size)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "\n",
    "            try:\n",
    "                name_parts = fname.split('_')\n",
    "                age = float(name_parts[0])\n",
    "                gender = int(name_parts[1])\n",
    "                race = int(name_parts[2])\n",
    "                if gender not in [0, 1] or race not in [0, 1, 2, 3, 4]:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            images.append(img)\n",
    "            ages.append(age)\n",
    "            genders.append(gender)\n",
    "            races.append(race)\n",
    "\n",
    "        return np.array(images), {\n",
    "            'age_output': np.array(ages, dtype=np.float32),\n",
    "            'gender_output': np.array(genders, dtype=np.int32),\n",
    "            'race_output': np.array(races, dtype=np.int32)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c97485-e328-4700-bd30-7321e63a0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import (Input, Dense, GlobalAveragePooling2D, \n",
    "                                     Dropout, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "\n",
    "# Optional: Focal loss for gender\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n",
    "        bce_exp = tf.exp(-bce)\n",
    "        return alpha * (1 - bce_exp) ** gamma * bce\n",
    "    return loss_fn\n",
    "\n",
    "# Create the model\n",
    "def create_model(input_shape=(96, 96, 3), num_races=5):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomContrast(0.1)\n",
    "    ])\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = data_augmentation(inputs)\n",
    "\n",
    "    base = MobileNetV2(include_top=False, input_shape=input_shape, weights='imagenet')\n",
    "    base.trainable = True\n",
    "    x = base(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Age Head\n",
    "    x_age = Dense(128, activation='relu')(x)\n",
    "    x_age = Dropout(0.3)(x_age)\n",
    "    x_age = Dense(64, activation='relu')(x_age)\n",
    "    age_output = Dense(1, name='age_output')(x_age)\n",
    "\n",
    "    # Gender Head\n",
    "    x_gender = Dense(128, activation='relu')(x)\n",
    "    x_gender = BatchNormalization()(x_gender)\n",
    "    x_gender = Dropout(0.4)(x_gender)\n",
    "    x_gender = Dense(64, activation='relu')(x_gender)\n",
    "    gender_output = Dense(1, activation='sigmoid', name='gender_output')(x_gender)\n",
    "\n",
    "    # Race Head\n",
    "    x_race = Dense(128, activation='relu')(x)\n",
    "    x_race = BatchNormalization()(x_race)\n",
    "    x_race = Dropout(0.4)(x_race)\n",
    "    race_output = Dense(num_races, activation='softmax', name='race_output')(x_race)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[age_output, gender_output, race_output])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "        loss={\n",
    "            'age_output': Huber(),\n",
    "            'gender_output': focal_loss(),\n",
    "            'race_output': 'sparse_categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'age_output': 2.0,\n",
    "            'gender_output': 2.0,\n",
    "            'race_output': 1.0\n",
    "        },\n",
    "        metrics={\n",
    "            'age_output': ['mae'],\n",
    "            'gender_output': ['accuracy'],\n",
    "            'race_output': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f877a9b-c470-4f5e-b689-516117c3c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Usage\n",
    "folder =  r'C:\\ageimg'\n",
    "file_list = [f for f in os.listdir(folder) if f.endswith('.jpg') or f.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d621a09-b588-4f3d-8345-183d520bab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(file_list)\n",
    "split = int(len(file_list) * 0.8)\n",
    "train_files = file_list[:split]\n",
    "val_files = file_list[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe80c76-4c34-43d5-9b83-d70e2c404d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = AgeGenderRaceDataGenerator(train_files, folder)\n",
    "val_gen = AgeGenderRaceDataGenerator(val_files, folder, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c7cb30-8e8a-4fcb-b22b-ad4e0fdeb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(patience=3, factor=0.5, verbose=1)\n",
    "]\n",
    "\n",
    "# Usage (ensure you have train_gen and val_gen ready):\n",
    "model = create_model()\n",
    "\n",
    "# Optional: class weights if gender imbalance exists\n",
    "gender_class_weight = {0: 1.0, 1: 2.0}  # Example\n",
    "class_weights = {'gender_output': gender_class_weight}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "625a57a6-3773-437b-8982-6abc73e65600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m863s\u001b[0m 1s/step - age_output_loss: 25.6599 - age_output_mae: 26.1533 - gender_output_accuracy: 0.5863 - gender_output_loss: 0.0688 - loss: 53.5202 - race_output_accuracy: 0.3057 - race_output_loss: 2.0622 - val_age_output_loss: 10.0941 - val_age_output_mae: 10.5801 - val_gender_output_accuracy: 0.6613 - val_gender_output_loss: 0.0629 - val_loss: 21.7013 - val_race_output_accuracy: 0.4941 - val_race_output_loss: 1.3856 - learning_rate: 5.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 1s/step - age_output_loss: 9.5268 - age_output_mae: 10.0090 - gender_output_accuracy: 0.6709 - gender_output_loss: 0.0475 - loss: 20.6428 - race_output_accuracy: 0.4675 - race_output_loss: 1.4941 - val_age_output_loss: 7.6623 - val_age_output_mae: 8.1388 - val_gender_output_accuracy: 0.7021 - val_gender_output_loss: 0.0424 - val_loss: 16.6389 - val_race_output_accuracy: 0.5690 - val_race_output_loss: 1.2297 - learning_rate: 5.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 789ms/step - age_output_loss: 8.6141 - age_output_mae: 9.0936 - gender_output_accuracy: 0.6966 - gender_output_loss: 0.0409 - loss: 18.6611 - race_output_accuracy: 0.5231 - race_output_loss: 1.3511 - val_age_output_loss: 7.4683 - val_age_output_mae: 7.9435 - val_gender_output_accuracy: 0.7635 - val_gender_output_loss: 0.0315 - val_loss: 16.0799 - val_race_output_accuracy: 0.6125 - val_race_output_loss: 1.0792 - learning_rate: 5.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 995ms/step - age_output_loss: 8.3441 - age_output_mae: 8.8218 - gender_output_accuracy: 0.7197 - gender_output_loss: 0.0382 - loss: 17.9914 - race_output_accuracy: 0.5618 - race_output_loss: 1.2261 - val_age_output_loss: 7.8530 - val_age_output_mae: 8.3298 - val_gender_output_accuracy: 0.7654 - val_gender_output_loss: 0.0310 - val_loss: 16.8899 - val_race_output_accuracy: 0.6098 - val_race_output_loss: 1.1265 - learning_rate: 5.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m656s\u001b[0m 1s/step - age_output_loss: 7.6596 - age_output_mae: 8.1361 - gender_output_accuracy: 0.7230 - gender_output_loss: 0.0370 - loss: 16.5636 - race_output_accuracy: 0.5810 - race_output_loss: 1.1711 - val_age_output_loss: 8.3836 - val_age_output_mae: 8.8551 - val_gender_output_accuracy: 0.7652 - val_gender_output_loss: 0.0317 - val_loss: 17.9743 - val_race_output_accuracy: 0.5974 - val_race_output_loss: 1.1559 - learning_rate: 5.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810ms/step - age_output_loss: 7.3256 - age_output_mae: 7.8016 - gender_output_accuracy: 0.7417 - gender_output_loss: 0.0344 - loss: 15.8571 - race_output_accuracy: 0.5969 - race_output_loss: 1.1380\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 970ms/step - age_output_loss: 7.3257 - age_output_mae: 7.8018 - gender_output_accuracy: 0.7417 - gender_output_loss: 0.0344 - loss: 15.8574 - race_output_accuracy: 0.5969 - race_output_loss: 1.1380 - val_age_output_loss: 8.2284 - val_age_output_mae: 8.7035 - val_gender_output_accuracy: 0.7756 - val_gender_output_loss: 0.0299 - val_loss: 17.6265 - val_race_output_accuracy: 0.6111 - val_race_output_loss: 1.1203 - learning_rate: 5.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 995ms/step - age_output_loss: 7.0617 - age_output_mae: 7.5353 - gender_output_accuracy: 0.7507 - gender_output_loss: 0.0335 - loss: 15.2915 - race_output_accuracy: 0.6090 - race_output_loss: 1.1020 - val_age_output_loss: 7.3902 - val_age_output_mae: 7.8586 - val_gender_output_accuracy: 0.7909 - val_gender_output_loss: 0.0282 - val_loss: 15.9020 - val_race_output_accuracy: 0.6119 - val_race_output_loss: 1.0814 - learning_rate: 2.5000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m547s\u001b[0m 907ms/step - age_output_loss: 6.9497 - age_output_mae: 7.4227 - gender_output_accuracy: 0.7579 - gender_output_loss: 0.0330 - loss: 15.0284 - race_output_accuracy: 0.6223 - race_output_loss: 1.0629 - val_age_output_loss: 7.6833 - val_age_output_mae: 8.1524 - val_gender_output_accuracy: 0.7895 - val_gender_output_loss: 0.0284 - val_loss: 16.5289 - val_race_output_accuracy: 0.6069 - val_race_output_loss: 1.1204 - learning_rate: 2.5000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 810ms/step - age_output_loss: 6.6481 - age_output_mae: 7.1213 - gender_output_accuracy: 0.7535 - gender_output_loss: 0.0326 - loss: 14.4165 - race_output_accuracy: 0.6164 - race_output_loss: 1.0548 - val_age_output_loss: 7.4090 - val_age_output_mae: 7.8809 - val_gender_output_accuracy: 0.7953 - val_gender_output_loss: 0.0279 - val_loss: 15.9721 - val_race_output_accuracy: 0.6046 - val_race_output_loss: 1.1111 - learning_rate: 2.5000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 829ms/step - age_output_loss: 6.6706 - age_output_mae: 7.1420 - gender_output_accuracy: 0.7588 - gender_output_loss: 0.0319 - loss: 14.4463 - race_output_accuracy: 0.6300 - race_output_loss: 1.0417\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 960ms/step - age_output_loss: 6.6706 - age_output_mae: 7.1420 - gender_output_accuracy: 0.7588 - gender_output_loss: 0.0319 - loss: 14.4462 - race_output_accuracy: 0.6300 - race_output_loss: 1.0417 - val_age_output_loss: 8.1281 - val_age_output_mae: 8.5975 - val_gender_output_accuracy: 0.7861 - val_gender_output_loss: 0.0281 - val_loss: 17.4315 - val_race_output_accuracy: 0.6036 - val_race_output_loss: 1.1375 - learning_rate: 2.5000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m572s\u001b[0m 949ms/step - age_output_loss: 6.3374 - age_output_mae: 6.8091 - gender_output_accuracy: 0.7667 - gender_output_loss: 0.0313 - loss: 13.7573 - race_output_accuracy: 0.6332 - race_output_loss: 1.0189 - val_age_output_loss: 7.2037 - val_age_output_mae: 7.6737 - val_gender_output_accuracy: 0.7878 - val_gender_output_loss: 0.0281 - val_loss: 15.4995 - val_race_output_accuracy: 0.6208 - val_race_output_loss: 1.0495 - learning_rate: 1.2500e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 897ms/step - age_output_loss: 6.3019 - age_output_mae: 6.7732 - gender_output_accuracy: 0.7651 - gender_output_loss: 0.0310 - loss: 13.6874 - race_output_accuracy: 0.6340 - race_output_loss: 1.0216 - val_age_output_loss: 7.2504 - val_age_output_mae: 7.7213 - val_gender_output_accuracy: 0.7903 - val_gender_output_loss: 0.0276 - val_loss: 15.5848 - val_race_output_accuracy: 0.6200 - val_race_output_loss: 1.0403 - learning_rate: 1.2500e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 853ms/step - age_output_loss: 6.2629 - age_output_mae: 6.7325 - gender_output_accuracy: 0.7743 - gender_output_loss: 0.0306 - loss: 13.5792 - race_output_accuracy: 0.6470 - race_output_loss: 0.9933 - val_age_output_loss: 7.0469 - val_age_output_mae: 7.5170 - val_gender_output_accuracy: 0.7932 - val_gender_output_loss: 0.0272 - val_loss: 15.1525 - val_race_output_accuracy: 0.6287 - val_race_output_loss: 1.0178 - learning_rate: 1.2500e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 790ms/step - age_output_loss: 6.2732 - age_output_mae: 6.7441 - gender_output_accuracy: 0.7787 - gender_output_loss: 0.0307 - loss: 13.5922 - race_output_accuracy: 0.6502 - race_output_loss: 0.9845 - val_age_output_loss: 6.9743 - val_age_output_mae: 7.4428 - val_gender_output_accuracy: 0.7934 - val_gender_output_loss: 0.0274 - val_loss: 15.0031 - val_race_output_accuracy: 0.6337 - val_race_output_loss: 1.0128 - learning_rate: 1.2500e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 785ms/step - age_output_loss: 6.1753 - age_output_mae: 6.6466 - gender_output_accuracy: 0.7777 - gender_output_loss: 0.0300 - loss: 13.4111 - race_output_accuracy: 0.6441 - race_output_loss: 1.0000 - val_age_output_loss: 7.0813 - val_age_output_mae: 7.5508 - val_gender_output_accuracy: 0.7967 - val_gender_output_loss: 0.0275 - val_loss: 15.2346 - val_race_output_accuracy: 0.6277 - val_race_output_loss: 1.0290 - learning_rate: 1.2500e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 787ms/step - age_output_loss: 6.1016 - age_output_mae: 6.5754 - gender_output_accuracy: 0.7854 - gender_output_loss: 0.0296 - loss: 13.2471 - race_output_accuracy: 0.6503 - race_output_loss: 0.9807 - val_age_output_loss: 6.9443 - val_age_output_mae: 7.4097 - val_gender_output_accuracy: 0.8002 - val_gender_output_loss: 0.0272 - val_loss: 14.9622 - val_race_output_accuracy: 0.6256 - val_race_output_loss: 1.0344 - learning_rate: 1.2500e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m539s\u001b[0m 893ms/step - age_output_loss: 5.9944 - age_output_mae: 6.4656 - gender_output_accuracy: 0.7848 - gender_output_loss: 0.0295 - loss: 13.0258 - race_output_accuracy: 0.6541 - race_output_loss: 0.9749 - val_age_output_loss: 7.2050 - val_age_output_mae: 7.6730 - val_gender_output_accuracy: 0.8056 - val_gender_output_loss: 0.0268 - val_loss: 15.4845 - val_race_output_accuracy: 0.6268 - val_race_output_loss: 1.0360 - learning_rate: 1.2500e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 985ms/step - age_output_loss: 5.9813 - age_output_mae: 6.4485 - gender_output_accuracy: 0.7814 - gender_output_loss: 0.0302 - loss: 12.9871 - race_output_accuracy: 0.6585 - race_output_loss: 0.9678 - val_age_output_loss: 7.2072 - val_age_output_mae: 7.6738 - val_gender_output_accuracy: 0.8046 - val_gender_output_loss: 0.0264 - val_loss: 15.4802 - val_race_output_accuracy: 0.6289 - val_race_output_loss: 1.0269 - learning_rate: 1.2500e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 950ms/step - age_output_loss: 5.9521 - age_output_mae: 6.4220 - gender_output_accuracy: 0.7879 - gender_output_loss: 0.0289 - loss: 12.9265 - race_output_accuracy: 0.6503 - race_output_loss: 0.9642\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 1s/step - age_output_loss: 5.9520 - age_output_mae: 6.4219 - gender_output_accuracy: 0.7879 - gender_output_loss: 0.0289 - loss: 12.9262 - race_output_accuracy: 0.6504 - race_output_loss: 0.9641 - val_age_output_loss: 7.0426 - val_age_output_mae: 7.5083 - val_gender_output_accuracy: 0.8063 - val_gender_output_loss: 0.0265 - val_loss: 15.1349 - val_race_output_accuracy: 0.6314 - val_race_output_loss: 1.0113 - learning_rate: 1.2500e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m657s\u001b[0m 1s/step - age_output_loss: 5.8394 - age_output_mae: 6.3096 - gender_output_accuracy: 0.7829 - gender_output_loss: 0.0297 - loss: 12.6870 - race_output_accuracy: 0.6582 - race_output_loss: 0.9483 - val_age_output_loss: 7.0227 - val_age_output_mae: 7.4885 - val_gender_output_accuracy: 0.8065 - val_gender_output_loss: 0.0266 - val_loss: 15.0926 - val_race_output_accuracy: 0.6316 - val_race_output_loss: 1.0089 - learning_rate: 6.2500e-06\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d50889c0-df6d-4992-9bde-da86f88747e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = model.evaluate(train_gen, verbose=0)\n",
    "val_metrics = model.evaluate(val_gen, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad5d9e8b-0839-475a-96de-0f535cdcc99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics from evaluation\n",
    "metric_names = model.metrics_names\n",
    "train_results = model.evaluate(train_gen, return_dict=True, verbose=0)\n",
    "val_results = model.evaluate(val_gen, return_dict=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8c30d09-bb40-4745-a23f-ede9d3cad672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Final Metrics:\n",
      "Train - Gender Acc: 0.8115 | Race Acc: 0.6519 | Age MAE: 6.01\n",
      "Valid - Gender Acc: 0.8002 | Race Acc: 0.6256 | Age MAE: 7.41\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n📊 Final Metrics:\")\n",
    "print(f\"Train - Gender Acc: {train_results['gender_output_accuracy']:.4f} | Race Acc: {train_results['race_output_accuracy']:.4f} | Age MAE: {train_results['age_output_mae']:.2f}\")\n",
    "print(f\"Valid - Gender Acc: {val_results['gender_output_accuracy']:.4f} | Race Acc: {val_results['race_output_accuracy']:.4f} | Age MAE: {val_results['age_output_mae']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faf2555a-1f7a-4fcd-b7d3-f9b30d38b177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Prediction Results\n",
      "🧒 Age: 76.7\n",
      "🚻 Gender: Female\n",
      "🌍 Race: White\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rites\\AppData\\Local\\Temp\\ipykernel_12028\\3056239620.py:36: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  age = round(float(age_pred[0]) , 1)  # Denormalize if needed\n",
      "C:\\Users\\rites\\AppData\\Local\\Temp\\ipykernel_12028\\3056239620.py:37: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  gender = gender_labels[int(gender_pred[0] > 0.5)]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Modify these according to your dataset\n",
    "race_labels = ['White', 'Black', 'Asian', 'Indian', 'Other']\n",
    "gender_labels = ['Male', 'Female']\n",
    "\n",
    "def predict_from_image(img_path, model, max_age=100):\n",
    "    \"\"\"\n",
    "    Predict age, gender, and race from an image using the provided model.\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): Path to the image file.\n",
    "        model (tf.keras.Model): Trained Keras model.\n",
    "        max_age (int): Max age used during normalization (default 100).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Predictions including age, gender, and race.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"❌ Error: Could not load image from path: {img_path}\")\n",
    "\n",
    "    # Preprocess\n",
    "    img = cv2.resize(img, (96, 96))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    age_pred, gender_pred, race_pred = model.predict(img, verbose=0)\n",
    "\n",
    "    # Decode\n",
    "    age = round(float(age_pred[0]) , 1)  # Denormalize if needed\n",
    "    gender = gender_labels[int(gender_pred[0] > 0.5)]\n",
    "    race = race_labels[np.argmax(race_pred[0])]\n",
    "\n",
    "    # Print\n",
    "    print(\"🎯 Prediction Results\")\n",
    "    print(f\"🧒 Age: {age}\")\n",
    "    print(f\"🚻 Gender: {gender}\")\n",
    "    print(f\"🌍 Race: {race}\")\n",
    "\n",
    "    return {\"age\": age, \"gender\": gender, \"race\": race}\n",
    "preds = predict_from_image(\"istockphoto-154946755-612x612.jpg\", model, max_age=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e022358-e9e9-4697-861e-6b626b14dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"age_gender_race_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245cf04-ad15-492f-9dd3-aac4b1da7a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c85ac-9c3e-4435-99b0-a3f40242d53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
